
"""
Given either
1. some root directories to scan for .bdqc cache files and/or
2. aggregate JSON file(s)

...scan all the JSON. In both cases (which can be arbitrarily interleaved)
scanning consumes one file's JSON at a time. Identify all features of
interest according to configuration and build tmp files containing vectors.

Concurrently maintain in memory the vector of filenames consumed and a map
from statistic names to the tmp files containing them.

Whenever a new statistic is identified its vector must be padded with
placeholders for all the already-scanned files that did not contain it.

Heuristic analysis

Truly context-free discovery is vacuous; it can't happen. However, as soon
as we have *some* information about a file (within a set that we a priori
believe to be "similar")--for example, that one of the files contains a
table--we immediately have significant context to use. Presence of data
generated by a specific plugin (especially table) immediately suggests a set
of heuristics, so it might make sense to implement:
1. generic (plugin-agnostic) heuristics and
2. plugin-specific heuristics

Question: should plugin-specific heuristics be packaged in the plugin that
generates the data?

Plugins may produce arbitrarily complex results--anything expressable as
JSON which permits complex deep-structured data. For the purposes of
meta-analysis, two constraints are added:
1. Objects must never be empty
2. Arrays (matrix structures built from nested Arrays) must be of
	homogeneous type.

We can automatically do deep inspection, flattening heirarchies but doing
this generically is probably unwise. Again table is a good example: the
table plugin's results include arrays of objects, but the array count
counts will differ. However, 

	"SIMILARITY" OF TWO COMPLEX-STRUCTURED FILES IMPLIES IDENTITY OF 
	SOME SUBSET OF STATISTICS AT MULTIPLE LEVELS."
	In particular, matrix dimensions

Within categorical data the only criteria that might make sense universally
is unanimity. User configuration might then merely specific what statistics
it makes sense to apply unanimity to, e.g. column counts, level sets within
categorical columns, etc.

1. All files should have some results from the same set of plugins; a
	file entirely missing data from 1 or more plugins is already aberrant.
	That is, existence of data for plugin P can be treated as a categorical
	(boolean) statistic

2. Presence of data for a specific plugin (such as tabular) or at any given
	level of a deep structure

Tables:
	a. ought to have same structure
		i. same column count
		ii. categorical columns ought to have same level sets

Candidate heuristics/constraints
1. all categorical (string) scalar statistis are identical
2. all categorical scalar statistics (in plugin) are identical
3. a specific statistic should be { unanimous, equal-to-value }
4. all quantitative statistics should not be outliers
5. all 1D string arrays *are* sets
6. bounds on quantitative vars or set cardinalities

statistic_path := ( plugin, root_name, [ descendent_name1, descendent_name2, ...] )

No supplied paths => all top-level scalars
heuristic( statistic_path ) => statistical_class for statistic_path

apply heuristic to...
	any statpath with appropriate inferred statistic
	{statpath1, statpath2, ... }
"""

import sys
import os
import os.path
import json
import tempfile

import bdqc.dir

from bdqc.statistic import Descriptor

_EMPTY_MATRIX_TYPE = ((0,),None)

_MSG_BAD_ARRAY  = "Arrays must constitute matrices"
_MSG_BAD_OBJECT = "Objects must not be empty"

if __debug__:
	_is_scalar = lambda v:\
		isinstance(v,int) or \
		isinstance(v,float) or \
		isinstance(v,str)

def _json_matrix_type( l ):
	"""
	Returns a pair (dimensions,element_type) where 
	1. dimensions is a tuple of integers giving each dimension's size, and
	2. element_type is an integer type code.
	For example, if f is a float in...

		[ [[f,f,f,f],[f,f,f,f]],
		  [[f,f,f,f],[f,f,f,f]],
		  [[f,f,f,f],[f,f,f,f]] ]

	...then the return is...

	( (3,2,4), Descriptor.TY_FLOAT )

	Note that TY_NONSCALAR is a legitimate element type, so...
		[ [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ] ]

	...is ( (4,3), Descriptor.TY_NONSCALAR )

	Returns None if either:
	1. the matrix is not complete--there is inconsistency in dimensions, or
	2. the element type is not identical throughout.

	Warning: this function is recursive; mind your stack!
	"""
	if not ( isinstance(l,list) and len(l) > 0 ):
		return False
	if all([isinstance(elem,list) for elem in l]):
		types = [ _json_matrix_type(elem) for elem in l ]
		# None percolates up...
		if any([elem is None for elem in types]) or \
			len(frozenset(types)) > 1:
			return None
		else:
			return ( (len(l),)+types[0][0], types[0][1] )
	else: # l is a list that contains AT LEAST ONE non-list
		types = [ Descriptor.scalar_type(elem) for elem in l ]
		# If there are multiple types it's not a matrix at all (by our
		# restrictied definition in this code)
		if len(frozenset(types)) > 1:
			return None
		else:
			return ( (len(types),), types[0] ) 


class Cache(object):
	"""
	Run-length encodes objects
	"""
	def __init__( self, placeholder_count ):
		self.store = tempfile.SpooledTemporaryFile(mode="w+")
		self.count = placeholder_count
		self.last  = None
		self.flushes = 0

	def __call__( self, value ):
		"""
		Just increment the count of consecutive identical values if
		"""
		if self.count > 0:
			if self.last != value:
				# ...the flush.
				print( "!{}\t{}".format( self.count, repr(value) ), file=self.store )
				self.count = 0
				self.last = value
				self.flushes += 1
		else:
			self.last = value
		self.count += 1


class MetaAnalysis(object):
	"""
	Contains a configuration consisting of
	1. heuristics to be applied
	2. paths to complex multi-level plugins
	...which guide accumulation of data from a combination of aggregate JSON
	files and directory scanning.
	Heuristics to be applied drive extraction: any 1st level statistic of a
	type for which a heuristic has been specified is extracted. In addition,
	deeper statistics.
	First-level univariate statistics for which a heuristic exists are
	automatically extracted. Other statistics more deeply nested in cached
	or aggregated analyses are extracted only if mentioned by name.
	"""
	def __init__( self ):
		"""
		"""
		# Initialize the cache map
		self.cache = {}
		self.files = []

	def __call__( self, filename, analysis:"JSON data represented as Python objects"=None ):
		"""
		Process all the data associated with a single primary file.

		If <filename>'s analysis was aggregated into a larger JSON file,
		then <analysis> will be provided, having been parsed out by the
		caller. Otherwise, we need to load the analysis from <filename>
		which should end in ".bdqc". More specifically, analysis will be
		None when this is called from a directory walker
		"""
		# Load the JSON data from name if it wasn't provided...
		if analysis is None:
			if not (
				filename.endswith(".bdqc") or
				filename.endswith(".json") ):
				raise RuntimeError("expected \".bdqc\" cache file, received "+filename )
			with open( filename ) as fp:
				analysis = json.loads( fp )
			filename = filename[:-5] # snip off the suffix.
		# ...then analyze it.
		# _visit calls self.accum for each non-traversable element in the
		# JSON content. (See elsewhere for what "non-traversable" means.)
		self._visit( analysis )
		self.files.append( filename )

	def _addstat( self, statname, value, meta=None ):
		"""
		"""
		# Insure a cache exists for the statistic
		try:
			cache = self.cache[ statname ]
		except KeyError:
			cache = Cache(len(self.files))
			self.cache[ statname ] = cache
		# ...then cache the value(s)
		if meta:
			# If it's a vector (1D matrix) of strings assume it's a set.
			if len(meta[0]) == 1 and meta[1] == Descriptor.TY_STR:
				cache( set(value) )
			else:
				# It's either multi-dimensional or it's element type
				# is Numeric or non-scalar.
				cache( meta )
		else:
			assert _is_scalar( value )
			cache( value )

	def _visit( self, data ):
		"""
		Traverse a JSON object, identifying:
		1. scalars (numeric or strings)
		2. matrices
		Matrices are DEFINED as nested Arrays of a single-type (which may itself
		be an Object). Scalar matrices are nested Arrays of a scalar type.
		Nested arrays in which the leaves are not of uniform type are not
		considered matrices.

		++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
		Matrices of Object type are subject to further traversal, but matrices
		of scalar type are treated as leaves.
		++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

		One-dimensional matrices are, of course, vectors, and when their content
		is String they may be interpreted as sets.

		The root element (data) is always going to be a JSON Object (Python
		dict) mapping plugin names to the data each plugin produced.
		"""
		assert data and isinstance( data, dict ) # a non-empty dict
		# Using iteration rather than recursion.
		path = []
		stack = []
		node = data
		i = iter(node.items())
		while node:
			push = None
			# Move forward in the current compound value (object or array)
			if isinstance(node,dict): # representing a JSON Object
				try:
					k,v = next(i)
					if isinstance(v,dict):
						if not len(v) > 0:
							raise RuntimeError(_MSG_BAD_OBJECT)
						push = ( k, i )   # NO callback.
					else: # It's a scalar or list, each of which *always*
						# ...trigger a callback.
						pa = '/'.join( path+[k,] )
						if isinstance(v,list):
							# Matrices MAY be empty, but the element type
							# of an empty matrix cannot be determined.
							mt = _json_matrix_type(v) if len(v) > 0 else _EMPTY_MATRIX_TYPE
							if mt is None:
								raise RuntimeError(_MSG_BAD_ARRAY)
							self._addstat( pa, v, mt )
							# If there are nested Objects, descend...
							if mt[1] == 0 and len(v) > 0:
								push = ( k, i )
						else:
							assert _is_scalar(v)
							self._addstat( pa, v )
				except StopIteration:
					node = None
			else: # node represents a JSON Array
				assert isinstance(node,list) and isinstance(i,int)
				if i < len(node):
					v = node[i]
					if isinstance(v,dict):
						if not len(v) > 0:
							raise RuntimeError(_MSG_BAD_OBJECT)
						push = ( str(i), i+1 ) # NO callback.
					else: # It's a scalar or list, each of which *always*
						# ...trigger a callback.
						pa = '/'.join( path+[str(i),] )
						if isinstance(v,list):
							# Matrices MAY be empty, but the element type
							# of an empty matrix cannot be determined.
							mt = _json_matrix_type(v) if len(v) > 0 else _EMPTY_MATRIX_TYPE
							if mt is None:
								raise RuntimeError(_MSG_BAD_ARRAY)
							self._addstat( pa, v, mt )
							# If there are nested Objects, descend...
							if mt[1] == 0 and len(v) > 0:
								push = ( str(i), i+1 )
							else:
								i += 1
						else:
							assert _is_scalar(v)
							self._addstat( pa, v )
							i += 1
				else:
					node = None
			# If v is not an empty compound value, a scalar, or a matrix--in
			# other words, if v is further traversable, push the current node
			# onto the stack and start traversing v. (Depth-first traversal.)
			if push:
				assert (isinstance(v,list) or isinstance(v,dict)) and len(v) > 0
				path.append( push[0] )
				stack.append( (node,push[1]) )
				i = iter(v.items()) if isinstance(v,dict) else 0
				node = v
			# If we've exhausted the current node, pop something off the stack
			# to resume traversing. (Clearly, exhaustion is mutually-exclusive
			# of encountering a new tranversable. Thus, the elif...)
			elif (node is None) and stack:
				path.pop(-1)
				node,i = stack.pop(-1)
		# end _visit

	def analyze( self ):
		pass

	def dump( self, prefix="." ):
		n = 0
		for k in sorted(self.cache.keys()):
			ifp = self.cache[k]
			ifp.seek(0)
			with open( os.path.join( prefix, "{:04d}.txt".format(n) ), "w" ) as ofp:
				print( "#", k, file=ofp )
				line = ifp.readline()
				while len(line) > 0:
					ofp.write( line )
					line = ifp.readline()
			n += 1

def _main( args ):

	a = MetaAnalysis()

	for s in args.sources:
		if os.path.isdir( s ):
			# Look for "*.bdqc" files under "s/" each of which contains
			# *one* file's analysis as a single JSON object.
			# dir.walk calls a visitor with the filename
			bdqc.dir.walk( s, args.depth, args.include, args.exclude, a )
		elif os.path.isfile( s ):
			# s is assumed to contain a ("pre") aggregated collection of analyses
			# of multiple files.
			with open(s) as fp:
				for filename,content in json.load( fp ).items():
					a( filename, content )
		else:
			raise RuntimeError( "{} is neither file nor directory".format(s) )
	a.dump( 'tmp' )


if __name__=="__main__":
	import argparse

	_parser = argparse.ArgumentParser(
		description="A framework for \"Big Data\" QC/validation.",
		epilog="""The command line interface is one of two ways to use this
		framework. Within your own Python scripts you can create, configure
		and programmatically run a bdqc.Executor.""")
#	_parser.add_argument( "--plugins", "-p",
#		default=None, metavar="PLUGINLIST",
#		help="""Plugins to execute EITHER in addition to OR instead of the
#		defaults listed in {}. The argument is a comma-separated
#		list of names. Each name must be EITHER the fully-qualified name of
#		a Python3 package implementing a plugin, OR the name of a file which
#		itself contains a list of plugins, one per line.
#		If %(metavar)s is prefixed with "+", the listed plugins AUGMENT
#		the defaults; otherwise the listed plugins REPLACE the defaults.
#		Note that a given plugin's dependencies will ALWAYS be added to
#		whatever you specify.
#		""".format( DEFAULT_PLUGIN_RCFILE ) )

	# Directory recursion options

	_parser.add_argument( "--depth", "-d",
		default=None, type=int,
		help="Maximum depth of recursion when scanning directories.")
	_parser.add_argument( "--include", "-I",
		default=None,
		help="""When recursing through directories, only files matching the
		<include> pattern are included in the analysis. The pattern should
		be a valid Python regular expression and usually should be enclosed
		in single quotes to protect it from interpretation by the shell.""")
	_parser.add_argument( "--exclude", "-E",
		default=None,
		help="""When recursing through directories, any files matching the
		<exclude> pattern are excluded from the analysis. The comments
		regarding the <include> pattern also apply here.""")

	_parser.add_argument( "sources", nargs="+",
		help="""Files, directories and/or manifests to analyze. All three
		classes of input may be freely intermixed with the caveat that the
		names of manifests (files listing other files) should be prefixed
		by \"@\" on the command line.""" )

	_args = _parser.parse_args()

	# If inclusion or exclusion patterns were specified, just verify they're
	# valid regular expressions here. The result of compilation is discarded
	# now. This is just an opportunity for Python to report a regex format
	# exception before entering directory recursion.

	if _args.include:
		re.compile( _args.include )
	if _args.exclude:
		re.compile( _args.exclude )

	_main( _args )

