
"""
This module aggregates all statistics generated by "within-file" analysis,
flattening the JSON data into a matrix.

It then applies the heuristics to identify potentially anomalous files:
1. quantitative data is expected to be centrally-distributed, so robust
	outlier detection is applied to quantitative data
2. categorical data and compound data types are expected to be identical
	across files

----------------------------------------------------------------------------
Aggregation
----------------------------------------------------------------------------

Given either
1. some root directories to scan for .bdqc cache files and/or
2. aggregate JSON file(s)

...scan all the JSON. In both cases (which can be arbitrarily interleaved)
scanning consumes one file's JSON at a time. Identify all features of
interest according to configuration and build tmp files containing vectors.

Concurrently, maintain in memory
1. the vector of filenames consumed and
2. a map from statistic names to the tmp files containing them.

Whenever a new statistic is first encountered its (tempfile-resident) vector
must be padded with placeholders for all the already-scanned files that did
not contain it. Ideally, when a set of presumed-similar files are in fact
similar, this should never happen.

At completion of the above we have a matrix, each column of which resides in
a file, and a map from the originally-analyzed files to the rows of the
matrix to which they correspond.

******** Complex data types ********

The process of flattening can itself be adjusted, particularly with respect
to its treatment of complex data types. TODO: explore this.

This code inserts one type of column that does not *directly* correspond to
original data: the matrix descriptor.

----------------------------------------------------------------------------
Heuristic analysis
----------------------------------------------------------------------------

Files that are a priori expected to be "similar" should be *identical* with
respect to specific characteristics. For example, files that are known to
contain tabular data typically should have identical column counts. (This
need not *always* apply, though; that is why it is a heuristic.) Moreover,
if one of those columns contains labels (i.e. categorical data) all files
might be expected to contain the same *set* of labels in that column.

Quantitative (i.e. numerical, floating-point) data is rarely expected to
manifest identity because of noise inherent in the phenomena and/or its
measurement. However, quantitative data from "similar" sources may be
expected to exhibit a central tendency: to be dispersed about its mean.
For example, files containing genome sequencing results of a different
individual of the same species, performed on the same sequencing platform,
etc. should typically be *approximately* the same size (in terms of bytes).

In the context of BDQC wherein the within-file analysis runs a dynamically-
determined set of plugins, one might expect every file (if they're truly
"similar") to have received identical "treatment" by plugins--that is, the
same set of plugins was selected to run on every file. This implies that no
*.bdqc file is missing any statistic (JSON path) that is present in one or
more other *.bdqc files.

Column selectors:
	path match, e.g.
		"/x/y/[p,q-r]/z,<type>
Constraints:
	column_cardinality <comparison> <int>
	value_cardinality <comparison> <int>
	value <comparison> <values>
	has_gaussian_outliers
"""

import sys
import os
import os.path
import json
import tempfile
import array


import bdqc.dir
from bdqc.statistic import Descriptor
import bdqc.builtin.compiled

# TODO: Investigate what makes sense for this.
# It relies on the performance of the median couple.
MIN_CARD_IMPLYING_QUANTITATIVE_INTEGER = 26

STATUS_NOTHING_TO_SEE = 0
STATUS_MISSING_VALUES = 1
STATUS_MULTIPLE_TYPES = 2
STATUS_ANOMALIES      = 3

STATUS = [
	"no anomalies detected",
	"missing values present in plugin output",
	"conflicts in the column types",
	"anomalies detected"
]

# Matrices may be traverse:
# 1. always
# 2. only if they have scalar component type
# 3. only if they have object component type
# 4. or never.
_ALWAYS_TRAVERSE = True
_EMPTY_MATRIX_TYPE = ((0,),None)

# These are the only additional constraints we put on plugin-generated JSON.
_MSG_BAD_ARRAY  = "Arrays must constitute matrices (N complete dimensions with uniform component type)"
_MSG_BAD_OBJECT = "Objects must not be empty"

if __debug__:
	_is_scalar = lambda v:\
		isinstance(v,int) or \
		isinstance(v,float) or \
		isinstance(v,str)

def _json_matrix_type( l ):
	"""
	Returns a pair (dimensions,element_type) where 
	1. dimensions is a tuple of integers giving each dimension's size, and
	2. element_type is an integer type code.
	For example, if f is a float in...

		[ [[f,f,f,f],[f,f,f,f]],
		  [[f,f,f,f],[f,f,f,f]],
		  [[f,f,f,f],[f,f,f,f]] ]

	...then the return is...

	( (3,2,4), Descriptor.TY_FLOAT )

	Note that TY_NONSCALAR is a legitimate element type, so...
		[ [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ],
		  [ {...}, {...}, {...} ] ]

	...is ( (4,3), Descriptor.TY_NONSCALAR )

	Returns None if either:
	1. the matrix is not complete--there is inconsistency in dimensions, or
	2. the element type is not identical throughout.

	Warning: this function is recursive; mind your stack!
	"""
	if not ( isinstance(l,list) and len(l) > 0 ):
		return False
	if all([isinstance(elem,list) for elem in l]):
		types = [ _json_matrix_type(elem) for elem in l ]
		# None percolates up...
		if any([elem is None for elem in types]) or \
			len(frozenset(types)) > 1:
			return None
		else:
			return ( (len(l),)+types[0][0], types[0][1] )
	else: # l is a list that contains AT LEAST ONE non-list
		types = [ Descriptor.scalar_type(elem) for elem in l ]
		# If there are multiple types it's not a matrix at all (by our
		# restrictied definition in this code)
		if len(frozenset(types)) > 1:
			return None
		else:
			return ( (len(types),), types[0] ) 


class Cache(object):
	"""
	Caches one matrix column's data in a tempfile.
	Writing to (and even opening) the file is deferred until two distinct
	values are witnessed.
	"""
	def __init__( self, placeholder_count=0 ):
		"""
		This cache's initial state is equivalent to that which would result
		from <placeholder_count> invocations of self.__call__( None ).
		"""
		self.store = None
		self.last  = None
		self.count   = placeholder_count
		self.missing = placeholder_count
		# The counts in self.types are only valid after _flush.
		self.types = {'b':0,'i':0,'f':0,'s':0}

	def __del__( self ):
		if self.store:
			self.store.close()

	def __str__( self ):
		assert self.store is not None
		types = "b:{b},i:{i},f:{f},s:{s}".format( **self.types )
		return "{}/{}/{}".format( self.count, self.missing, types )

	def __len__( self ):
		return self.count

	def __call__( self, value ):
		"""
		Just count identical values until a second distinct value is
		observed. When a second distinct value is observed, create the
		tempfile. Subsequently, all values are written directly to the
		file.
		"""
		_incTypeCount(value)
		if self.store:
			self._write( value )
		elif self.count > 0:
			if self.last != value:
				self._flush()
				self._write( value )
		else:
			self.last = value
		self.count += 1

	def _incTypeCount( self, value ):
		if value is None:
			self.missing += 1
		else:
			if not ( isinstance(value,float) \
				or isinstance(value,int) \
				or isinstance(value,str) \
				or isinstance(value,bool) ):
				raise TypeError( "unexpected type: " + type(value).__name__ )
			self.types[ type(value).__name__[0] ] += 1

	def _write( self, value ):
		"""
		To insure that string values of "None" aren't confused with the
		None type, replace None by a more literal representation of None:
		an empty line.
		"""
		print( repr(value) if value is not None else "", file=self.store )

	def _flush( self ):
		"""
		"""
		assert self.store is None # because this should only be called once!
		self.store = tempfile.SpooledTemporaryFile( mode="w+" )
		for i in range(self.count):
			self._write( self.last )

	def data( self ):
		"""
		Idempotent--reload if and only if data hasn't been loaded.
		"""
		if self._data is None:
			assert self.store.tell() > 0
			self.store.seek(0)
			# Remember: None is represented as empty lines. See _write.
			self._data = [ eval(l.rstrip()) if len(l.rstrip()) > 0 else None
				for l in self.store.readlines() ]
			# We won't be using the tempfile again, so...
			self.store.close()
			self.store = None
		return self._data

	def _non_missing_len( self ):
		return self.count - self.missing

	def _make_value_histogram( self, min_card=0 ):
		"""
		Create dict mapping values to their cardinality within self.data().

		If min_card is non-zero, caller is only interested in whether or
		not the cardinality of distinct values is >= min_card. In this
		case, we may exit before finishing enumeration of the data and,
		thus, before finishing computation of the histogram.

		If min_card is zero, the histogram is fully computed and left in
		self.
		"""
		self.value_histogram = dict()
		for v in self.data():
			if v is not None:
				try:
					self.value_histogram[ v ] += 1
				except KeyError:
					self.value_histogram[ v ]  = 1
			if min_card > 0 and len(self.value_histogram) >= min_card:
				# Since the histogram is incomplete, if we need it later
				# we'll have to recomputed it. So don't leave it around.
				delattr(self,value_histogram)
				return min_card
		return len(self.value_histogram)

	def add_missing_to_insure_count( self, expected_count ):
		"""
		This method insures that missing data is noted.
		If the last file processed by Matrix did not contain the
		statistic held by this cache, self.__call__ was not called and
		this cache's total count of statistics will be expected_count-1.
		"""
		if len(self) < expected_count:
			self( None )
			return True
		return False

	def is_numeric( self ):
		"""
		...if int and float items account for all non-missing items.
		Note that quantitative implies numeric, but numeric does NOT
		imply quantitative.
		"""
		return self._non_missing_len() == (self.types['i'] + self.types['f'])

	def is_quantitative( self ):
		"""
		If *any* cached values were floating-point, the entire column will
		be treated as floating-point. That is, integers will be coerced.
		This is to accomodate the possibility that floating-point data that
		happens to occasionally have integral values may be emitted by the
		plugins as integers.

		If all non-missing values were integral AND the cardinality of the
		values is sufficiently large the column will be treated as quanti-
		tative for the purpose of anomaly detection.
		"""
		
		if self.types['f'] > 0: # Presence of floats implies...
			return True  # ...quantitative, but...
		elif not self.is_numeric(): # ...complete absence of numbers...
			return False # ...precludes quantitative.
		else: # Otherwise, we need the cardinality of integral values.
			assert all([ v is None or isinstance(v,int)
				for v in self.data() ])
			n = MIN_CARD_IMPLYING_QUANTITATIVE_INTEGER
			return self._make_value_histogram( n ) >= n

	def is_missing_data( self ):
		"""
		Returns the count of missing data--that is, a count of values
		of None--in this cache.
		"""
		return self.missing > 0

	def is_uniquely_typed( self ):
		"""
		Indicates whether or this cache contains exactly one of boolean,
		string, or numeric data. Missing data is ignored.

		Note that integral and floating-point are treated as a single
		type for the purposes of this method. More specifically, if any
		floating-point values are present, integral values will be treated
		as floating-point.
		"""
		return self.store is None \
			or self.is_numeric() \
			or sum([ int(v > 0) for v in self.types.values()]) == 1

	def is_single_valued( self ):
		"""
		Determine whether or not this column is *effectively* single-valued.

		By "effectively" it is meant that EITHER:
		1. only a single value (of any type) was observed, OR
		2. all (non-missing) values were numeric and they were sufficiently
			centrally-distributed without outliers.

		This method compute as little as possible to arrive at an answer.
		However, in the case of quantitative data, the answer actually
		requires determination of the presence of outliers, and thus the
		identities of the aberrant elements.

		For non-quantitative data more work will be required later if
		the identities of aberrant elements is sought.
		"""
		if self.store is None: # self.last was never flushed because only...
			return True        # ...one value was ever seen. Case closed!
		if self.is_quantitative():
			# Filter missing values and force floating-point types.
			data = [ float(v) for v in filter( lambda x:x is not None, self.data()) ]
			array_data = array.array("d", data )
			lb,ub = bdqc.builtin.compiled.robust_bounds( array_data )
			self.aberrant = [ i for i in range(len(data)) if
				(data[i] is not None) 
				and (data[i] < lb or data[i] > ub) ]
			self.lb = lb
			self.ub = ub
			# self.density = bdqc.builtin.compiled.gaussian_kde( array_data )
			return len(self.aberrant) == 0

		# Computation within is_quantitative sometimes yields a partial or
		# complete cardinality assessment as a side effect...

		if hasattr(self,"value_histogram"):
			# If it's *not* quantitative, but a histogram exists, then the
			# data *was* exhaustively enumerated...
			assert sum(self.value_histogram.values()) == self._non_missing_len()
			return len(self.value_histogram) > 1

		return self._make_value_histogram() > 1

	def aberrant_indices( self ):
		"""
		Return a list of the indices of aberrant elements.

		The "aberrant elements": 
		1. in quantitative data are the outliers.
		2. in non-quantitative data are whichever elements have the
			value of smallest cardinality (the minority).

		This method should only be called if this Cache is known to contain
		multiple values. (Otherwise, state assumed to exist won't!)
		"""
		assert hasattr(self,"aberrant") \
			or hasattr(self,"value_histogram")
		# ...otherwise, if neither of these exist, we could just make the
		# tail of this method conditional on "not is_single_valued" which
		# would trigger their generation.
		if hasattr(self,"aberrant"):
			return self.aberrant
		else:
			min_cardinality = min( self.value_histogram.values() )
			minority_values = frozenset( filter(
				lambda k:self.value_histogram[k] == min_cardinality,
				self.value_histogram.keys() ) )
			return [ i for i in range(len(self.column)) 
				if self.column[i] in minority_values ]


class Matrix(object):
	"""
	Holds the "flattened" results of the "within-file" analysis carried out
	by plugins on a set of data files.
	Each column corresponds to one of the statistics produced by one of the
	plugins.
	"""
	def __init__( self, config:"a list of column filters"=[] ):
		"""
		"""
		assert isinstance(config,list)
		self.config = config
		# Initialize the column map
		self.column = {}
		self.files  = []
		self.rejects = set()

	def __call__( self, filename, analysis:"JSON data represented as Python objects" ):
		"""
		Process all the (JSON) data generated by the within-file analysis
		of a single primary file.
		Upon exit from this method, each of self.column should be exactly
		one datum longer.

		Returns a boolean indicating whether or not the addition of filename
		resulted in missing values anywhere. (The latest addition might have
		had statistics earlier files didn't, or vica versa.)
		This boolean provides a way to exit file analysis early in (the
		relatively uninteresting) case of missing statistics.
		"""
		# _visit calls self._addstat for each non-traversable element in
		# JSON content. (See elsewhere for what "non-traversable" means.)
		# This will increase the lengths of *some* of self.column by 1.
		columns_on_entry = len(self.column)
		self._visit( analysis )
		self.files.append( filename )
		# Insure all columns have the same length (by inserting missing data
		# placeholders where necessary) before exiting.
		missing_added = 0
		for c in self.column.values():
			if c.add_missing_to_insure_count( len(self.files) ):
				missing_added += 1
		assert all([ len(c)==len(self.files) for c in self.column.values() ])
		return missing_added > 0 or (
			columns_on_entry > 0 and len(self.column) > columns_on_entry )
 

	def _accept( self, statname ):
		"""
		We only create columns for statistics that match path selectors
		unless any heuristic in use applies to all columns (which most
		of the default do). TODO:revist this.
		"""
		return True

	def _addstat( self, statname, value, meta=None ):
		"""
		Actually write a datum to the appropriate column.
		Called by _visit every time it reaches:
		1. a leaf node (necessarily a scalar) or
		2. a JSON Array (which might represent an arbitrary dimensional
		   matrix or a set).
		The latter case allows:
		1. emission of matrix metadata 
		2. emission of potential sets as values
		...which are mutually exclusive. As long as these possibilities
		*are* mutually exclusive we don't need any additional path
		differentiators--that is, we can just use the JSON path.

		Returns:
			Boolean value for which True means "descend into the matrix."
		(A True is only meaningful if the node is, in fact, a JSON Array).
		"""
		descend = (meta is not None) and (_ALWAYS_TRAVERSE or meta[1] == 0)
		# Insure a column exists for the statistic...
		try:
			column = self.column[ statname ]
		except KeyError:
			# ...IF AND ONLY IF it:
			# 1. hasn't already been rejected, and
			# 2. passes path-based filters that define stats of interest.
			if statname in self.rejects:
				return descend
			if not self._accept( statname ):
				self.rejects.add( statname )
				return descend # ...without caching anything. 
			# Otherwise, create a new Cache for statname...
			column = Cache(len(self.files))
			self.column[ statname ] = column
		# ...then append the value(s)
		if meta:
			# If it's a vector (1D matrix) of strings and...
			if len(meta[0]) == 1 and meta[1] == Descriptor.TY_STR:
				S = set(value)
				# ...the cardinality of the set of values equals the count
				# of values, then encode it as a set.
				if len(S) == meta[0][0]:
					column( S )
					descend = False
					# ...because the list was just treated as a *value*.
				else:
					column( meta )
			else:
				# It's either multi-dimensional or it's element type
				# is Numeric or non-scalar.
				column( meta )
		else:
			assert value is None or _is_scalar( value )
			column( value )
			descend = False
		return descend

	def _visit( self, data ):
		"""
		Traverse a JSON object, identifying:
		1. scalars (numeric or strings)
		2. matrices
		Matrices are DEFINED as nested Arrays of a single-type (which may itself
		be an Object). Scalar matrices are nested Arrays of a scalar type.
		Nested arrays in which the leaves are not of uniform type are not
		considered matrices.

		Traversal of matrices may be made conditional on the component type.

		One-dimensional matrices are, of course, vectors, and when their content
		is String they may be interpreted as sets.

		The root element (data) is always going to be a JSON Object (Python
		dict) mapping plugin names to the data each plugin produced.
		"""
		assert data and isinstance( data, dict ) # a non-empty dict
		# Using iteration rather than recursion.
		path = []
		stack = []
		node = data
		i = iter(node.items())
		while node:
			push = None
			# Move forward in the current compound value (object or array)
			if isinstance(node,dict): # representing a JSON Object
				try:
					k,v = next(i)
					if isinstance(v,dict):
						if not len(v) > 0:
							raise RuntimeError(_MSG_BAD_OBJECT)
						push = ( k, i )   # NO callback.
					else: # It's a scalar or list, each of which *always*
						# ...trigger a callback.
						pa = '/'.join( path+[k,] )
						if isinstance(v,list):
							# Matrices MAY be empty, but the element type
							# of an empty matrix cannot be determined.
							mt = _json_matrix_type(v) if len(v) > 0 else _EMPTY_MATRIX_TYPE
							if mt is None:
								raise RuntimeError(_MSG_BAD_ARRAY)
							
							# If there are nested Objects (or we're traversing
							# all), descend...
							if self._addstat( pa, v, mt ) and len(v) > 0:
								push = ( k, i )
						else:
							assert v is None or _is_scalar(v)
							self._addstat( pa, v )
				except StopIteration:
					node = None
			else: # node represents a JSON Array
				assert isinstance(node,list) and isinstance(i,int)
				if i < len(node):
					v = node[i]
					if isinstance(v,dict):
						if not len(v) > 0:
							raise RuntimeError(_MSG_BAD_OBJECT)
						push = ( str(i), i+1 ) # NO callback.
					else: # It's a scalar or list, each of which *always*
						# ...trigger a callback.
						pa = '/'.join( path+[str(i),] )
						if isinstance(v,list):
							# Matrices MAY be empty, but the element type
							# of an empty matrix cannot be determined.
							mt = _json_matrix_type(v) if len(v) > 0 else _EMPTY_MATRIX_TYPE
							if mt is None:
								raise RuntimeError(_MSG_BAD_ARRAY)
							
							# If there are nested Objects (or we're traversing
							# all), descend...
							if self._addstat( pa, v, mt ) and len(v) > 0:
								push = ( str(i), i+1 )
							else:
								i += 1
						else:
							assert v is None or _is_scalar(v)
							self._addstat( pa, v )
							i += 1
				else:
					node = None
			# If v is not an empty compound value, a scalar, or a matrix--in
			# other words, if v is further traversable, push the current node
			# onto the stack and start traversing v. (Depth-first traversal.)
			if push:
				assert (isinstance(v,list) or isinstance(v,dict)) and len(v) > 0
				path.append( push[0] )
				stack.append( (node,push[1]) )
				i = iter(v.items()) if isinstance(v,dict) else 0
				node = v
			# If we've exhausted the current node, pop something off the stack
			# to resume traversing. (Clearly, exhaustion is mutually-exclusive
			# of encountering a new tranversable. Thus, the elif...)
			elif (node is None) and stack:
				path.pop(-1)
				node,i = stack.pop(-1)
		# end _visit

	def _create_incidence( self ):
		"""
		"""

	def analyze( self ):
		"""
		1. Identify columns with missing data.
		2. Identify columns for which a single type could not be inferred.
		3. Identify columns with:
			a. multiple values if non-quantitative
			b. outliers if quantitative.

		Missing data implies that the same set of (dynamically determined)
		plugins (or parts of plugins) were not run on all subject files and
		probably indicates inadequately filtered subject files. It is
		impossible in this case to say *which* files (if any) are the source.

		Each column should unambiguously typed as one of the following:
		1. boolean
		2. string
		3. integer
		4. floating-point (possibly with some integers)

		Failure to infer a unique type for a column is probably due to
		either plugin design or flagrant software bugs, and, again, it is
		impossible to identify "guilty" files from this.

		Finally, assuming no data is missing and all columns are uniquely
		typed, each column should contain:
		1. a tightly distributed set of quantitative values (no outliers) or
		2. a single non-quantitative value
		Creates a sparse representation of an incidence matrix.
		"""
		if any([ c.is_missing_data() for c in self.column.values() ]):
			return STATUS_MISSING_VALUES
		if any([ not c.is_uniquely_typed() for c in self.cache.values() ]):
			return STATUS_MULTIPLE_TYPES

		# The preceding two cases are the uninteresting cases.

		self.anom_stat = sorted( list( filter(
			lambda k: not self.column[k].is_single_valued(),
			self.cache.keys() ) ) )
		if len(self.anom_stat) > 0:
			return STATUS_ANOMALIES

		return STATUS_NOTHING_TO_SEE

	def dump( self, prefix="." ):
		n = 0
		for k in sorted(self.cache.keys()):
			ifp = self.cache[k].fileptr()
			ifp.seek(0)
			with open( os.path.join( prefix, "{:04d}.txt".format(n) ), "w" ) as ofp:
				print( "#", k, file=ofp )
				line = ifp.readline()
				while len(line) > 0:
					ofp.write( line )
					line = ifp.readline()
				print( "#", str(self.cache[k]), sep="", file=ofp )
			n += 1


class _Loader(object):
	"""
	Matrix is intended to be __call__'ed directly by the run method
	of bdqc.scan.Executor. In particular, it wants a (filename,JSON data)
	pair.
	For the use case where an ad hoc collection of *.bdqc files is
	collected independently via directory recursion, we need a means to
	convert the callbacks from bdqc.dir.walk (that only contain filenames)
	into the required (filename,data) pairs.
	This is the sole purpose of this private class.
	Technically, it is providing a closure.
	"""
	def __init__( self, target, expected_ext=[".bdqc",".json"] ):
		self.target = target
		self.expected_ext = expected_ext

	def _is_valid_filename( self, filename ):
		return any([ filename.endswith(ext) for ext in self.expected_ext ])

	def __call__( self, filename ):
		if not self._is_valid_filename( filename ):
			raise RuntimeError("expected \".bdqc\" cache file, received "+filename )
		with open( filename ) as fp:
			analysis = json.loads( fp )
		basename = os.path.splitext(filename)[0] # snip off the suffix.
		self.target( basename, analysis )


def analyze( args ):
	"""
	Aggregate JSON into a Matrix then call the Matrix' analyze method.
	This function allows 
	"""
	m = Matrix(None)
	for s in args.sources:
		if os.path.isdir( s ):
			# Look for "*.bdqc" files under "s/" each of which contains
			# *one* file's analysis as a single JSON object.
			# dir.walk calls a visitor with the filename
			bdqc.dir.walk( s, args.depth, args.include, args.exclude, 
				_Loader( m ) )
		elif os.path.isfile( s ):
			# s is assumed to contain a ("pre") aggregated collection of analyses
			# of multiple files.
			with open(s) as fp:
				for filename,content in json.load( fp ).items():
					m( filename, content )
		else:
			raise RuntimeError( "{} is neither file nor directory".format(s) )
	if __debug__:
		m.dump( 'tmp' )
	m.analyze()


if __name__=="__main__":
	import argparse

	_parser = argparse.ArgumentParser(
		description="A framework for \"Big Data\" QC/validation.",
		epilog="""The command line interface is one of two ways to use this
		framework. Within your own Python scripts you can create, configure
		and programmatically run a bdqc.Executor.""")

	# Directory recursion options

	_parser.add_argument( "--depth", "-d",
		default=None, type=int,
		help="Maximum depth of recursion when scanning directories.")
	_parser.add_argument( "--include", "-I",
		default=None,
		help="""When recursing through directories, only files matching the
		<include> pattern are included in the analysis. The pattern should
		be a valid Python regular expression and usually should be enclosed
		in single quotes to protect it from interpretation by the shell.""")
	_parser.add_argument( "--exclude", "-E",
		default=None,
		help="""When recursing through directories, any files matching the
		<exclude> pattern are excluded from the analysis. The comments
		regarding the <include> pattern also apply here.""")

	_parser.add_argument( "--config", "-c",
		default=None,
		help="""Load a heuristic configuration from the given file.
		This configuration entirely replaces the defaults.""")

	_parser.add_argument( "sources", nargs="+",
		help="""Files, directories and/or manifests to analyze. All three
		classes of input may be freely intermixed with the caveat that the
		names of manifests (files listing other files) should be prefixed
		by \"@\" on the command line.""" )

	_args = _parser.parse_args()

	# If inclusion or exclusion patterns were specified, just verify they're
	# valid regular expressions here. The result of compilation is discarded
	# now. This is just an opportunity for Python to report a regex format
	# exception before entering directory recursion.

	if _args.include:
		re.compile( _args.include )
	if _args.exclude:
		re.compile( _args.exclude )

	analyze( _args )

