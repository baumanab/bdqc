
"""
This module performs the "between-file" analysis.

It aggregates all statistics generated by "within-file" analysis,
flattens the JSON data into a matrix, and applies heuristics to a
possibly-filtered set of the statistics.
"""

import sys
from os.path import isfile

from bdqc.statistic import Descriptor
from bdqc.column import Vector
from bdqc.report import HTML,Plaintext
from bdqc.statpath import Selector
from bdqc.data import flatten

# Warning: the values of these constants are used to index function pointer
# arrays. Don't change these without changing code that uses them!
STATUS_NO_OUTLIERS     = 0
STATUS_INCOMPARABLES   = 1
STATUS_AMBIGUOUS_STATS = 2
STATUS_NULL_OUTLIERS   = 3
STATUS_VALUE_OUTLIERS  = 4

STATUS_MSG = [
	"no anomalies detected",
	"files are incomparable",
	"ambiguous statistical types",
	"missing data (null) outliers detected",
	"value outliers detected"
]

class Matrix(object):
	"""
	Holds the "flattened" results of the "within-file" analysis carried out
	by plugins on a set of data files.
	Each column corresponds to one of the statistics produced by one of the
	plugins.
	Each row corresponds to an analyzed file.
	Result of analysis is a list of anomalous rows and columns such that:
	1. a column is anomalous if it contains any anomalous row, and
	2. a row is anomalous if its file is anomalous in any statistic (column)
	An incidence matrix can be constructed from these.
	"""

	def __init__( self, **kwargs ):
		"""
		"""
		self.include = list(kwargs.get("include", []))
		self.exclude = list(kwargs.get("exclude", []))
		# Matrix expects Selectors...
		assert all([ isinstance(s,Selector) for s in self.include ])
		assert all([ isinstance(s,Selector) for s in self.exclude ])
		# Initialize the column map
		self.column = {}
		self.files  = []
		self.rejects = set()
		# The incomparables set identifies columns (statistics) that were
		# not present in all included files' analysis for which placeholder
		# values of None were added within the include method to fill out
		# the matrix. Presence of incomparables really precludes more in-
		# depth analysis.
		self.incomparables = set()
		# Setting of the following attributes is deferred to the analyze
		# method:
		# self.status
		# self.anom_col
		# self.anom_row

	def _addstat( self, statname, value, meta=None ):
		"""
		Actually write a datum to the appropriate column, creating
		the column just-in-time if necessary.
		Only called by add_file_data.

		Returns:
			A list of newly-created column names. (Currently, this will
		always contain zero or one element, but eventually columns may
		be created for metadata as well.)
		"""
		assert meta is None # ...for now and probably forever.
		columns_created = []
		# Insure a column exists for the statistic...
		try:
			column = self.column[ statname ]
		except KeyError:
			# ...IF AND ONLY IF it:
			# 1. hasn't already been rejected, and
			# 2. passes path-based filters that define stats of interest.
			if statname in self.rejects:
				return columns_created
			if any([ s(statname) for s in self.exclude ]):
				self.rejects.add( statname )
				return columns_created
			# Otherwise, create a new Vector for statname...
			column = Vector(len(self.files))
			self.column[ statname ] = column
			columns_created.append( statname )
		# ...then append the value(s)
		column.push( value )
		return columns_created

	def add_file_data( self, filename, data:"within-file analysis results" ):
		"""
		Conditionally include the given file's within-file analysis in the
		across-file analysis.

		Upon exit from this method, the matrix is one row longer--that is,
		each of self.column should be exactly one datum longer.

		An important side effect of this method is that this is the only
		place that incomparable files can be detected. Files' results are
		allowed to contain null for statistics--a null value is not the
		same as a missing statistic, but every file should have some value
		for every statistic. Concretely, this means:
		1. The very first file added establishes the matrix' columns.
		2. If columns are added subsequently (to preserve the matrix'
		   dimensions), then files are incomparable.
		3.  If column padding is ever required (again, to preserve the 
		   matrix' dimensions because a file was missing statistics
		   presenct in the others), then files are incomparable.

		Returns a boolean indicating whether or not the file was actually
		included.
		"""
		data = flatten( data ) # ...the hierarchy is needed no more
		assert all([ isinstance(elem,tuple) and len(elem) >= 2 for elem in data ])
		# Each elem is ( name, value [, descriptor ] )
		# If inclusion Selectors have been specified then a file is included
		# iff its analysis contains data matching at least one of them.
		if self.include:
			data = list( filter( lambda nvd:any([ sel(nvd[0]) for sel in self.include ]), data ) )
		# If none of the include Selectors matched any of the data, the
		# filter entirely decimates data...
		if len(data) == 0:
			return False # file was not included
		for nvd in data:
			columns_created = self._addstat( *nvd )
			# If this is NOT the 1st file we shouldn't be adding columns!
			if len(self.files) > 0 and columns_created:
				self.incomparables.update( columns_created )
		self.files.append( filename )
		# Insure all columns have the same length (by inserting missing data
		# placeholders where necessary) before exiting.
		columns_padded = 0
		for k,c in self.column.items():
			if c.pad_to_count( len(self.files) ):
				columns_padded += 1
				self.incomparables.add( k )
		assert all([ len(c)==len(self.files) for c in self.column.values() ])
		return True

	def analyze( self ):
		"""
		1. Identify columns in which files are incomparable because
			not all files had results for the corresponding statistic.
		2. Identify columns for which a single type could not be inferred.
		3. Identify columns with missing data (values of None) NOT due to
			case #1. Missing data of this sort is anomalous by definition.
		4. Identify columns with *value* outliers:
			a. multiple values if non-quantitative
			b. outliers if quantitative.

		If no columns meet any of these criteria there is essentially
		nothing to report--everything is normal.

		The first two cases are relatively uninteresting; it is impossible
		to say which files are the anomalies.

		Each column should unambiguously typed as one of the following:
		1. boolean
		2. string
		3. integer
		4. floating-point (possibly with some integers)

		Failure to infer a unique type for a column is probably due to
		either plugin design or flagrant software bugs, and, again, it is
		impossible to identify "guilty" files from this.

		Finally, assuming no data is missing and all columns are uniquely
		typed, each column should contain:
		1. a tightly distributed set of quantitative values (no outliers) or
		2. a single non-quantitative value
		"""
		if self.incomparables:
			self.anom_col = sorted( list( self.incomparables ) )
			H = hash( self.column[self.anom_col[0]].missing_indices() )
			if all([hash(c.missing_indices()) == H for c in self.column.values() ]):
				# all statistics (columns) that are missing value, are
				# missing those value from the same set of files, so the
				# columns are collapsable. TODO
				pass # TODO: 
			self.anom_row = sorted( list( set().union(*[
				self.column[k].missing_indices() for k in self.anom_col ]) ) )
			self.status = STATUS_INCOMPARABLES
			return self.status

		self.anom_col = sorted( list( filter(
			lambda k: not self.column[k].is_uniquely_typed(),
			self.column.keys() ) ) )
		if len(self.anom_col) > 0:
			self.anom_row = sorted( list( set().union(*[
				self.column[k].minor_type_indices() for k in self.anom_col ]) ) )
			self.status = STATUS_AMBIGUOUS_STATS
			return self.status

		# The preceding two cases are the uninteresting cases.

		self.anom_col = sorted( list( filter(
			lambda k: self.column[k].is_missing_data(),
			self.column.keys() ) ) )
		if len(self.anom_col) > 0:
			self.anom_row = sorted( list( set().union(*[
				self.column[k].missing_indices() for k in self.anom_col ]) ) )
			self.status = STATUS_NULL_OUTLIERS
			return self.status

		self.anom_col = sorted( list( filter(
			lambda k: not self.column[k].is_single_valued(),
			self.column.keys() ) ) )

		if len(self.anom_col) > 0:
			# We have a list of column keys (names corresponding to
			# statistics) that are anomalous. Now generate a list of
			# all indices into the self.files list of files that
			# contain anomalies (by virtue of being included in any
			# column's anomaly list).
			self.anom_row = sorted( list( set().union(*[
				self.column[k].outlier_indices() for k in self.anom_col ]) ) )
			self.status = STATUS_VALUE_OUTLIERS
		else:
			self.status = STATUS_NO_OUTLIERS

		return self.status

	def incidence_matrix( self ):
		"""
		Return an incidence matrix the content of which depends on the
		nature of the anomalies (missing data, ambiguous types, or
		value discrepancies).
		"""
		body = []
		rows = []
		cols = []
		if self.status != STATUS_NO_OUTLIERS:
			rows_of_interest = ( # depend on analysis status
				None,
				Vector.present_indices,
				Vector.minor_type_indices,
				Vector.present_indices,
				Vector.outlier_indices)[ self.status ]
			is_row_of_interest = lambda rnum,cobj:rnum in rows_of_interest(cobj)
			body = [ [ is_row_of_interest(r, self.column[c] )
				for c in self.anom_col ]
				for r in self.anom_row ]
			rows = [ self.files[ r ] for r in self.anom_row ]
			cols = self.anom_col
		return { 'body':body, 'rows':rows, 'cols':cols }

	def status_msg( self ):
		return STATUS_MSG[ self.status ]

	def dump( self, fp=sys.stdout ):
		head = list(sorted(self.column.keys()))
		cols = [ self.column[k] for k in head ]
		print( "FILE", *head, sep="\t", file=fp )
		for r in range(len(self.files)):
			print( self.files[r], *[ c[r] for c in cols ], sep='\t', file=fp )


class _Loader(object):
	"""
	Matrix is intended to be __call__'ed directly by the run method
	of bdqc.scan.Executor. In particular, it wants a (filename,JSON data)
	pair.
	For the use case where an ad hoc collection of *.bdqc files is
	collected independently via directory recursion, we need a means to
	convert the callbacks from bdqc.dir.walk (that only contain filenames)
	into the required (filename,data) pairs.
	This is the sole purpose of this private class.
	Technically, it is providing a closure.
	"""
	def __init__( self, target, expected_ext=[".bdqc",".json"] ):
		self.target = target
		self.expected_ext = expected_ext

	def _is_valid_filename( self, filename ):
		return any([ filename.endswith(ext) for ext in self.expected_ext ])

	def __call__( self, filename ):
		if not self._is_valid_filename( filename ):
			raise RuntimeError("expected \".bdqc\" cache file, received "+filename )
		with open( filename ) as fp:
			analysis = json.loads( fp )
		basename = os.path.splitext(filename)[0] # snip off the suffix.
		self.target.add_file_data( basename, analysis )


def _main( args, output ):
	"""
	Aggregate JSON into a Matrix then call the Matrix' analyze method.
	This function allows 
	"""
	import os.path
	import json
	import bdqc.dir
	from bdqc.statpath import selectors

	statistic_filters = {}
	if args.use:
		statistic_filters["include"] = selectors(args.use )
	if args.ignore:
		statistic_filters["exclude"] = selectors(args.ignore)
	m = Matrix( **statistic_filters )
	for s in args.sources:
		if os.path.isdir( s ):
			# Look for "*.bdqc" files under "s/" each of which contains
			# *one* file's analysis as a single JSON object.
			# dir.walk calls a visitor with the filename
			bdqc.dir.walk( s, args.depth, args.include, args.exclude, 
				_Loader( m ) )
		elif isfile( s ):
			# s is assumed to contain a ("pre") aggregated collection of analyses
			# of multiple files.
			with open(s) as fp:
				for filename,content in json.load( fp ).items():
					m.add_file_data( filename, content )
		else:
			raise RuntimeError( "{} is neither file nor directory".format(s) )
	m.analyze()
	if m.status: # ...is other than STATUS_NO_OUTLIERS
		if args.report:
			report = args.report.lower()
			if report.startswith("text"):
				Plaintext(m).render( sys.stdout )
			elif report.startswith("html"):
				HTML(m).render( sys.stdout )
	if args.dump:
		with open(args.dump,"w") as fp:
			m.dump( fp )

if __name__=="__main__":
	import argparse

	_parser = argparse.ArgumentParser(
		description="A framework for \"Big Data\" QC/validation.",
		epilog="""The command line interface is one of two ways to use this
		framework. Within your own Python scripts you can create, configure
		and programmatically run a bdqc.Executor.""")

	# Directory recursion options

	_parser.add_argument( "--depth", "-d",
		default=None, type=int,
		help="Maximum depth of recursion when scanning directories.")

	# File filtering

	_parser.add_argument( "--include", "-I",
		default=None,
		help="""When recursing through directories, only files matching the
		<include> pattern are included in the analysis. The pattern should
		be a valid Python regular expression and usually should be enclosed
		in single quotes to protect it from interpretation by the shell.""")
	_parser.add_argument( "--exclude", "-E",
		default=None,
		help="""When recursing through directories, any files matching the
		<exclude> pattern are excluded from the analysis. The comments
		regarding the <include> pattern also apply here.""")

	# Statistic filtering

	_parser.add_argument( "--use",
		default=None,
		help="""Specify a list of statistics to use in heuristic
		analysis. This may either be a file containing one statistic
		per line, or a literal semi-colon-separated list of statistics.
		If this option is not given, all statistics (not listed in the
		ignore option) are candidates.""")
	_parser.add_argument( "--ignore",
		default=None,
		help="""Specify a list of statistics to ignore in heuristic
		analysis. This may either be a file containing one statistic
		per line, or a literal semi-colon-separated list of statistics.""")

	# Output options

	_parser.add_argument( "--dump",
		type=str,
		default="",
		help="""Dump the summary matrix as a table of tab-separated values
		to a file of the given name.""")
	_parser.add_argument( "--report",
		default=None,
		help="""Specify the type of analysis report desired. Must be
		one of {none,text,html}.""")

	_parser.add_argument( "sources", nargs="+",
		help="""Files, directories and/or manifests to analyze. All three
		classes of input may be freely intermixed with the caveat that the
		names of manifests (files listing other files) should be prefixed
		by \"@\" on the command line.""" )

	_args = _parser.parse_args()

	# If inclusion or exclusion patterns were specified, just verify they're
	# valid regular expressions here. The result of compilation is discarded
	# now. This is just an opportunity for Python to report a regex format
	# exception before entering directory recursion.

	if _args.include:
		re.compile( _args.include )
	if _args.exclude:
		re.compile( _args.exclude )

	_main( _args, sys.stdout )

